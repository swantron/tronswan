[
  {
    "id": "gcp-case-study-001",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They currently use Prometheus for monitoring and email alerts. They need to modernize monitoring and alerting. What GCP products should they move to?",
    "back": "Move to Cloud Monitoring (replaces Prometheus) for metrics collection and dashboards, Cloud Logging for log aggregation, Error Reporting for error tracking, and Pub/Sub + Cloud Functions for alerting (replaces email notifications). This provides centralized observability, better integration with GCP services, and more reliable alerting through multiple channels (SMS, Slack, PagerDuty via Pub/Sub subscribers).",
    "tags": ["case-study", "altostrat", "monitoring", "prometheus", "cloud-monitoring", "alerting"]
  },
  {
    "id": "gcp-case-study-002",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They need to modernize CI/CD for containerized deployments with centralized management. What should they move to from their current setup?",
    "back": "Move to Cloud Build for CI/CD pipelines (replaces Jenkins/GitLab CI), Artifact Registry for container image storage (replaces Docker Hub/private registry), Cloud Deploy for continuous delivery to GKE clusters, and optionally Anthos Config Management for multi-cluster configuration. This provides centralized CI/CD management across on-premises and cloud Kubernetes environments.",
    "tags": ["case-study", "altostrat", "cicd", "cloud-build", "artifact-registry", "cloud-deploy"]
  },
  {
    "id": "gcp-case-study-003",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They need AI-powered content summarization, metadata extraction, and inappropriate content detection. What specific GCP AI products should they use?",
    "back": "Use Vertex AI Generative AI Studio (Gemini models) for content summarization, Cloud Vision API for image metadata extraction, Cloud Video Intelligence API for video analysis and inappropriate content detection, Cloud Natural Language API for text analysis and metadata extraction, and Vertex AI Explainable AI for model interpretability. These replace manual content review processes.",
    "tags": ["case-study", "altostrat", "ai", "vertex-ai", "vision-api", "video-intelligence", "gen-ai"]
  },
  {
    "id": "gcp-case-study-004",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They need 24/7 natural language user support. What GCP product should they implement?",
    "back": "Implement Dialogflow CX for advanced chatbots with natural language understanding. Dialogflow CX provides conversational AI capabilities, integrates with their existing Google Identity, supports multi-language, and can be integrated into their web and mobile applications. This replaces or augments their current support system.",
    "tags": ["case-study", "altostrat", "dialogflow", "chatbot", "nlp", "customer-support"]
  },
  {
    "id": "gcp-case-study-005",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Retail: They currently use MySQL, Microsoft SQL Server, Redis, and MongoDB. What GCP database products should they migrate to?",
    "back": "Migrate MySQL to Cloud SQL for MySQL, migrate Microsoft SQL Server to Cloud SQL for SQL Server (or Cloud SQL for PostgreSQL if standardizing), migrate Redis to Memorystore for Redis, and migrate MongoDB to Firestore (NoSQL) or Cloud SQL for PostgreSQL (if relational model fits). These managed services provide high availability, automated backups, and easier scaling than self-managed databases.",
    "tags": ["case-study", "cymbal", "database-migration", "cloud-sql", "memorystore", "firestore"]
  },
  {
    "id": "gcp-case-study-006",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Retail: They use Grafana, Nagios, and Elastic for monitoring. What should they move to?",
    "back": "Move to Cloud Monitoring (replaces Grafana/Nagios for metrics and dashboards), Cloud Logging (replaces Elastic for log aggregation and search), and Error Reporting for application error tracking. This provides native GCP integration, better cost efficiency, and eliminates the need to manage open-source monitoring infrastructure.",
    "tags": ["case-study", "cymbal", "monitoring", "grafana", "nagios", "elastic", "cloud-monitoring"]
  },
  {
    "id": "gcp-case-study-007",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Retail: They need to replace SFTP file transfers and ETL batch processing. What GCP products should they use?",
    "back": "Replace SFTP with Cloud Storage (use Transfer Service for SFTP compatibility if partners require SFTP protocol), replace ETL batch processing with Dataflow for Apache Beam-based ETL pipelines, use Cloud Functions or Cloud Run for event-driven processing when files arrive, and Pub/Sub for event streaming. This eliminates manual SFTP processes and enables real-time or near-real-time data processing.",
    "tags": ["case-study", "cymbal", "sftp", "etl", "dataflow", "cloud-storage", "transfer-service"]
  },
  {
    "id": "gcp-case-study-008",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Retail: They need gen AI for product catalog enrichment (attributes, descriptions, images). What specific GCP products should they use?",
    "back": "Use Vertex AI Generative AI (Gemini models) for generating product attributes and descriptions from supplier data, use Imagen API for generating product image variations (colors, backgrounds, text overlays), and use Vertex AI Search for Retail (Discovery AI) to improve product discoverability. These replace manual catalog management processes.",
    "tags": ["case-study", "cymbal", "gen-ai", "vertex-ai", "imagen", "discovery-ai", "catalog-enrichment"]
  },
  {
    "id": "gcp-case-study-009",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Retail: They need conversational commerce with product discovery. What GCP products enable this?",
    "back": "Use Vertex AI Search for Retail (Discovery AI/Retail API) for intelligent product search and recommendations, use Dialogflow CX for virtual shopping assistants/chatbots, and integrate both into their web and mobile applications. Discovery AI processes natural language queries and returns relevant products, while Dialogflow handles conversational interactions. This replaces their custom web app catalog browsing and IVR system.",
    "tags": ["case-study", "cymbal", "discovery-ai", "dialogflow", "conversational-commerce", "retail-api"]
  },
  {
    "id": "gcp-case-study-010",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They're migrating from colocation. They have MySQL, MS SQL Server, Redis, and MongoDB. What GCP database products should they migrate to?",
    "back": "Migrate MySQL to Cloud SQL for MySQL, migrate MS SQL Server to Cloud SQL for SQL Server, migrate Redis to Memorystore for Redis, and migrate MongoDB to Firestore (if NoSQL fits) or Cloud SQL for PostgreSQL (if relational model works). All with high availability configurations for 99.9% uptime. These managed services eliminate database administration overhead.",
    "tags": ["case-study", "ehr", "database-migration", "cloud-sql", "memorystore", "colocation"]
  },
  {
    "id": "gcp-case-study-011",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They use open-source monitoring tools and email alerts. What should they move to for centralized visibility and proactive monitoring?",
    "back": "Move to Cloud Monitoring for metrics and dashboards (replaces open-source monitoring), Cloud Logging for log aggregation and search, Error Reporting for application errors, and Pub/Sub + Cloud Functions for alerting (replaces email alerts with SMS, Slack, PagerDuty, etc.). This provides centralized visibility, better integration, and more reliable alerting.",
    "tags": ["case-study", "ehr", "monitoring", "cloud-monitoring", "alerting", "observability"]
  },
  {
    "id": "gcp-case-study-012",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They use Microsoft Active Directory for user management. What GCP identity product should they use?",
    "back": "Use Cloud Identity (Google's identity and access management) or integrate Active Directory with Cloud Identity Sync to maintain AD while leveraging Cloud Identity features. Alternatively, use Cloud Identity for Google Workspace integration. Cloud Identity provides SSO, MFA, and integrates with Cloud IAM for fine-grained access control to GCP resources.",
    "tags": ["case-study", "ehr", "identity", "active-directory", "cloud-identity", "iam"]
  },
  {
    "id": "gcp-case-study-013",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They need to maintain legacy file- and API-based integrations with insurance providers on-premises. What GCP products enable secure hybrid connectivity?",
    "back": "Use Cloud VPN (IPsec) for secure site-to-site connectivity, or Dedicated Interconnect for higher bandwidth and lower latency, use Cloud Storage with Transfer Service for file-based integrations (replaces SFTP), use Apigee API Management or Cloud Endpoints for API integrations, and use VPC Service Controls to prevent data exfiltration. This maintains existing integrations while modernizing the backend.",
    "tags": ["case-study", "ehr", "hybrid-cloud", "vpn", "interconnect", "api-management", "vpc-service-controls"]
  },
  {
    "id": "gcp-case-study-014",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They need to achieve 99.9% availability for customer-facing systems. What specific GCP products and configurations provide this?",
    "back": "Use Cloud Load Balancing (multi-region) for global distribution, GKE with multiple node pools across zones with auto-scaling, Cloud SQL with high availability (multi-zone) configuration, Cloud Storage with multi-region buckets, Cloud CDN for static content caching, health checks configured on all services, and backup/restore procedures. Design for redundancy at every layer - compute, storage, networking, and databases.",
    "tags": ["case-study", "ehr", "high-availability", "multi-region", "load-balancing", "disaster-recovery"]
  },
  {
    "id": "gcp-case-study-015",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They have an outdated mainframe ERP system. What GCP database product should they migrate to?",
    "back": "Migrate to Cloud Spanner for globally distributed, horizontally scalable relational database (best for ERP requiring ACID transactions and global scale), or Cloud SQL for PostgreSQL if they don't need global distribution. Spanner provides 99.999% availability, automatic sharding, and strong consistency across regions, making it ideal for replacing mainframe ERP systems.",
    "tags": ["case-study", "knightmotives", "erp", "mainframe", "spanner", "database-migration"]
  },
  {
    "id": "gcp-case-study-016",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need vehicle connectivity and on-device AI processing for in-vehicle features. What GCP products should they use?",
    "back": "Use Cloud IoT Core for vehicle connectivity and device management (replaces custom connectivity solutions), use Edge TPU for on-device AI inference (reduces cloud dependency for rural areas), use Cloud Functions or Cloud Run for real-time cloud processing, and use Pub/Sub for vehicle data streaming. This enables reliable connectivity and offline-first AI capabilities.",
    "tags": ["case-study", "knightmotives", "iot", "edge-tpu", "cloud-iot-core", "edge-computing"]
  },
  {
    "id": "gcp-case-study-017",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need AI/ML infrastructure for autonomous vehicle development and training. What GCP products should they use?",
    "back": "Use Vertex AI for ML model training and deployment (replaces obsolete AI infrastructure), use AI Platform for custom ML training jobs, use Cloud Storage for storing simulation and training data, use BigQuery for analytics on vehicle data, and use Vertex AI Workbench for ML development. This provides scalable, managed ML infrastructure for autonomous vehicle development.",
    "tags": ["case-study", "knightmotives", "autonomous-vehicles", "vertex-ai", "ml", "ai-platform"]
  },
  {
    "id": "gcp-case-study-018",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need to monetize corporate data while ensuring EU data protection compliance. What GCP products enable secure data monetization?",
    "back": "Use BigQuery for data warehouse (with EU region data residency), use Cloud DLP for PII detection and de-identification before monetization, use Dataproc or Dataflow for data processing pipelines, use Pub/Sub for real-time data streams, use Apigee for API monetization (exposing data as APIs), use VPC Service Controls to prevent unauthorized data access, and ensure all processing occurs in EU regions for GDPR compliance.",
    "tags": ["case-study", "knightmotives", "data-monetization", "bigquery", "apigee", "gdpr", "cloud-dlp"]
  },
  {
    "id": "gcp-case-study-019",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need to modernize their unreliable online build-to-order system and create dealer tools. What GCP products should they use?",
    "back": "Use Cloud Run for the build-to-order web application (replaces unreliable system, provides auto-scaling), use Firestore for CRM and customer data (NoSQL, real-time updates), use Cloud SQL for inventory management (relational data), use Cloud Functions for order processing workflows, and use Cloud Monitoring + Cloud Logging for reliability monitoring. This provides scalable, reliable dealer and customer tools.",
    "tags": ["case-study", "knightmotives", "cloud-run", "firestore", "dealer-tools", "build-to-order"]
  },
  {
    "id": "gcp-case-study-020",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Retail: They need a Human-in-the-Loop (HITL) review system for gen AI-generated product content. What GCP products enable this workflow?",
    "back": "Use Vertex AI Generative AI APIs to generate content, store results in Cloud Storage or Firestore, use Cloud Run to host a review UI, use Cloud Workflows to orchestrate the HITL process (approve/reject/modify workflow), use Pub/Sub for event-driven notifications when content needs review, and update approved content in Cloud SQL/Firestore product catalog. This replaces manual catalog management.",
    "tags": ["case-study", "cymbal", "hitl", "gen-ai", "cloud-workflows", "human-review"]
  },
  {
    "id": "gcp-case-study-021",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They need to ingest and process data from new insurance providers quickly. What GCP products enable rapid provider onboarding?",
    "back": "Use Cloud Storage for file-based data ingestion (replaces legacy file systems), use Cloud Functions or Cloud Run for processing logic (event-driven, scalable), use Pub/Sub for event streaming, use Apigee API Management for API-based integrations (standardizes API contracts), use Dataflow for ETL processing, and use BigQuery for analytics and reporting. This enables faster onboarding than legacy systems.",
    "tags": ["case-study", "ehr", "data-ingestion", "apigee", "dataflow", "provider-onboarding"]
  },
  {
    "id": "gcp-case-study-022",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need network upgrades for plant connectivity and improved rural vehicle connectivity. What GCP networking products should they use?",
    "back": "Use Cloud VPN for secure site-to-site connectivity to manufacturing plants (replaces existing network), use Dedicated Interconnect for high-bandwidth, low-latency connections if needed, use Cloud CDN for content delivery to edge locations, and consider Anthos for edge computing in rural areas. For vehicles, use Cloud IoT Core with Edge TPU for offline-first capabilities in areas with poor connectivity.",
    "tags": ["case-study", "knightmotives", "networking", "vpn", "interconnect", "rural-connectivity"]
  },
  {
    "id": "gcp-case-study-023",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They're moving petabytes of video content from on-premises to Google Cloud. They need to serve content globally with lowest latency and minimum egress costs. The content is infrequently updated but frequently accessed. What architecture should they use?",
    "back": "Store all video files in a Multi-Region Cloud Storage bucket and enable Cloud CDN on an attached Global External HTTP(S) Load Balancer to cache the content globally. Cloud CDN caches content at edge locations worldwide, reducing latency and egress costs by serving cached content instead of fetching from Cloud Storage for each request. Multi-Region Cloud Storage provides high availability and low latency for cache misses.",
    "tags": ["case-study", "altostrat", "content-delivery", "cloud-storage", "cloud-cdn", "load-balancer", "cost-optimization"]
  },
  {
    "id": "gcp-case-study-024",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They receive massive, unpredictable bursts of real-time sensor data requiring immediate analysis for a live dashboard. The data must be transformed and loaded into a scalable, high-throughput database for GKE applications with low latency. What services should they use?",
    "back": "Use Cloud Pub/Sub for ingestion (handles massive bursts), Cloud Dataflow (Streaming) for real-time transformation and processing, and Cloud Bigtable for the high-throughput database. Cloud Bigtable provides low-latency reads/writes at scale, making it ideal for real-time dashboards. This architecture handles unpredictable traffic patterns and provides sub-second latency for dashboard queries.",
    "tags": ["case-study", "altostrat", "real-time", "pub-sub", "dataflow", "bigtable", "streaming"]
  },
  {
    "id": "gcp-case-study-025",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: Their GKE cluster's API server should not be accessible from the public internet, but must be reachable for management from on-premises via an existing Cloud VPN tunnel. What GKE configuration meets these requirements?",
    "back": "Deploy a Private GKE Cluster with the public endpoint disabled and configure Master Authorized Networks with the on-premises IP ranges. Private clusters have no public endpoint by default, and Master Authorized Networks restricts API server access to only specified IP ranges (the on-premises VPN ranges). This provides security while maintaining management access via VPN.",
    "tags": ["case-study", "altostrat", "gke", "security", "private-cluster", "vpn", "master-authorized-networks"]
  },
  {
    "id": "gcp-case-study-026",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: Their financial and customer data pipelines must prevent accidental or malicious exfiltration of sensitive information to unapproved public APIs (e.g., personal Cloud Storage buckets). What service enforces this security perimeter?",
    "back": "Use VPC Service Controls to create a perimeter around BigQuery and Cloud Storage projects, restricting data plane access to only resources inside the perimeter. VPC Service Controls prevents data exfiltration by blocking access to services outside the defined security boundary, even if IAM permissions would otherwise allow it. This is the definitive control plane solution for data exfiltration prevention.",
    "tags": ["case-study", "altostrat", "security", "vpc-service-controls", "data-exfiltration", "compliance"]
  },
  {
    "id": "gcp-case-study-027",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Altostrat Media: They're launching a new international platform requiring high-velocity writes globally (millions of users), OLTP workloads with strong consistency across all regions, and rapid automatic horizontal scaling without sharding or downtime. What database solution should they use?",
    "back": "Use Cloud Spanner configured as a multi-regional instance. Cloud Spanner is the only database that provides globally distributed, strongly consistent relational database with automatic horizontal scaling. It handles millions of writes globally, provides ACID transactions across regions, and scales automatically without manual sharding or downtime. This is ideal for global OLTP workloads requiring strong consistency.",
    "tags": ["case-study", "altostrat", "database", "spanner", "global", "strong-consistency", "scalability"]
  },
  {
    "id": "gcp-case-study-028",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Bank: They operate globally and must ensure all customer PII data is processed and stored exclusively within the customer's home jurisdiction (data residency). They also need a global, serverless data warehouse for aggregated, non-PII reporting. What architecture satisfies both requirements?",
    "back": "Store PII in separate, regionalized BigQuery datasets (ensures data residency in each jurisdiction). Create a BigQuery Authorized View in a central project to query aggregated, non-PII data across all regions. Authorized Views allow cross-region querying without physically moving data, maintaining data residency while enabling global reporting. This meets strict regulatory requirements while providing analytics capabilities.",
    "tags": ["case-study", "cymbal-bank", "data-residency", "bigquery", "authorized-view", "compliance", "pii"]
  },
  {
    "id": "gcp-case-study-029",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Bank: They're modernizing core banking with RTO less than 1 hour and RPO less than 15 minutes. The application uses GKE microservices and requires a strongly consistent transactional database. What services provide optimal resilience with least operational overhead?",
    "back": "Deploy on Regional GKE Autopilot (fully managed, highly available across zones, auto-scales). Use Cloud Spanner configured as a multi-regional instance (e.g., nam6 or eur3) with automated backups. Spanner provides strongly consistent, relational OLTP database with multi-regional availability, meeting strict RTO/RPO requirements. Autopilot reduces operational overhead while providing superior resilience.",
    "tags": ["case-study", "cymbal-bank", "high-availability", "rto", "rpo", "spanner", "gke-autopilot", "disaster-recovery"]
  },
  {
    "id": "gcp-case-study-030",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Bank: They need the central security team to manage organization-wide constraints and billing, while product teams under the 'Engineering' folder have full control over their own project resources (least privilege). What IAM setup meets these requirements?",
    "back": "Assign Organization Policy Administrator and Billing Account Administrator roles at the Organization level to the central security team. Assign Folder Admin role at the 'Engineering' folder level to product teams. This allows security to enforce org-wide policies and manage billing, while product teams can create and manage projects within their folder without broader organization access.",
    "tags": ["case-study", "cymbal-bank", "iam", "organization-policy", "billing", "least-privilege", "folder-admin"]
  },
  {
    "id": "gcp-case-study-031",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Bank: Production GKE workloads must reach Google services (Artifact Registry, BigQuery) using private IPs, but must be absolutely prevented from reaching external public internet or non-whitelisted Google Cloud services. What combination enforces this policy?",
    "back": "Use a Private GKE Cluster with Private Google Access enabled on the subnet. Implement VPC Service Controls perimeter around the production project's GKE and BigQuery services. Private GKE ensures no public internet access, Private Google Access enables private communication with Google APIs, and VPC Service Controls prevents data exfiltration to unauthorized services. This is the definitive solution for financial compliance.",
    "tags": ["case-study", "cymbal-bank", "security", "private-gke", "vpc-service-controls", "private-google-access", "compliance"]
  },
  {
    "id": "gcp-case-study-032",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "Cymbal Bank: They need a real-time fraud detection system analyzing millions of payment transactions per second. The system requires high-speed ingestion and transformation before feeding a streaming ML model. What architecture provides the most scalable, low-latency solution?",
    "back": "Use Cloud Pub/Sub for ingestion (handles millions of transactions/sec), Cloud Dataflow (Streaming) for real-time transformation and pre-processing, and Cloud Bigtable for serving the ML model. Cloud Dataflow provides serverless, complex real-time transformations, while Cloud Bigtable offers high-throughput, low-latency reads/writes perfect for streaming ML scoring. This architecture scales automatically and provides sub-second latency.",
    "tags": ["case-study", "cymbal-bank", "fraud-detection", "pub-sub", "dataflow", "bigtable", "streaming", "ml"]
  },
  {
    "id": "gcp-case-study-033",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They need hybrid connectivity to maintain legacy on-premises integrations while migrating containerized applications to Google Cloud. The solution must provide secure, high-performance connection with minimum 99.99% availability SLA and survive a single metro outage. What connectivity solution should they use?",
    "back": "Configure four Dedicated Interconnect connections: two in one metro and two in a second, geographically diverse metro, ensuring they terminate at different Google edge locations. Use dynamic routing (BGP) for automatic failover. Dedicated Interconnect provides 99.99% availability SLA and high performance. Geographic diversity across two metros ensures disaster recovery capability, surviving a complete metro failure.",
    "tags": ["case-study", "ehr", "hybrid-connectivity", "dedicated-interconnect", "disaster-recovery", "high-availability", "bgp"]
  },
  {
    "id": "gcp-case-study-034",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They store PHI in Cloud Storage used as a data lake for BigQuery analytics. PHI must be tokenized before non-privileged analysts query it, and all data leaving the project for external APIs must be strictly controlled for HIPAA compliance. What solution should they implement?",
    "back": "Use Cloud DLP for inspection, redaction, and de-identification (tokenization) of sensitive data before loading into BigQuery. Implement VPC Service Controls perimeter around BigQuery and Cloud Storage project services. Cloud DLP automates PHI tokenization, and VPC Service Controls prevents data exfiltration to unauthorized services. This is the Google-recommended solution for HIPAA compliance.",
    "tags": ["case-study", "ehr", "hipaa", "phi", "cloud-dlp", "vpc-service-controls", "tokenization", "compliance"]
  },
  {
    "id": "gcp-case-study-035",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: Their containerized customer-facing applications experience unpredictable, massive traffic spikes (e.g., during health advisories). They're currently on a zonal GKE cluster, causing capacity issues during spikes and high costs during low-traffic periods. What solutions optimize for both scalability and cost?",
    "back": "Migrate workloads to Regional GKE Autopilot cluster (provides HA, auto-scales, charges only for pod resources). Alternatively, refactor applications to run as Cloud Run services (scale-to-zero capability, rapid scaling, ultimate cost efficiency during low traffic). Both solutions handle unpredictable spikes automatically and optimize costs during low-traffic periods without manual intervention.",
    "tags": ["case-study", "ehr", "scalability", "cost-optimization", "gke-autopilot", "cloud-run", "scale-to-zero"]
  },
  {
    "id": "gcp-case-study-036",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: They need to modernize MySQL, MS SQL Server, and Redis databases. The new solution must support: 1) Low-latency reads globally, 2) Strong consistency for core patient record transactions, 3) Fully managed operations. What database solution should they use?",
    "back": "Use Cloud Spanner configured as a multi-regional instance. Cloud Spanner is the only database that provides global low-latency reads (multi-regional replication), strong consistency globally (required for patient records), and fully managed operations (automatic sharding, replication, failover). It meets all three requirements simultaneously, unlike regional Cloud SQL or eventually consistent NoSQL databases.",
    "tags": ["case-study", "ehr", "database", "spanner", "global", "strong-consistency", "patient-records"]
  },
  {
    "id": "gcp-case-study-037",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "EHR Healthcare: Their executive team is concerned that monitoring via various open-source tools is inconsistent and critical alerts are frequently missed. They need centralized, consistent view of system performance and proactive action based on specific log patterns across GKE and Anthos on-premises. What solution should they implement?",
    "back": "Use Anthos to provide consistent control plane for GKE and on-premises Kubernetes. Configure Cloud Logging to centralize all logs. Create a Log-based Metric on a critical error pattern, which triggers a Cloud Monitoring Alerting Policy. Log-based Metrics convert log patterns into measurable time series, and Monitoring Alerting Policies provide proactive notification. This is the standard method for centralized, proactive alerting.",
    "tags": ["case-study", "ehr", "monitoring", "cloud-logging", "log-based-metrics", "cloud-monitoring", "anthos", "alerting"]
  },
  {
    "id": "gcp-case-study-038",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need to ingest and process real-time telemetry data from over 2 million vehicles globally. A small, high-priority subset requires immediate, low-latency analysis for a fleet management dashboard, while the remaining large volume is compressed and uploaded later for batch processing. What architecture should they use?",
    "back": "Use Cloud IoT Core for vehicle connectivity and device management, Cloud Pub/Sub for high-throughput messaging, Cloud Dataflow (Streaming) for immediate transformations (filtering high-priority subset), and Cloud Bigtable for the real-time dashboard database. Cloud IoT Core integrates natively with Pub/Sub, Dataflow handles streaming transformations, and Bigtable provides low-latency reads/writes for the dashboard. Batch data can be stored separately in Cloud Storage for later processing.",
    "tags": ["case-study", "knightmotives", "iot", "telemetry", "pub-sub", "dataflow", "bigtable", "real-time"]
  },
  {
    "id": "gcp-case-study-039",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: Their vehicles run complex ML models at the edge for autonomous features. These models and local data caches (e.g., map data) must be reliably updated and synchronized with the central Google Cloud environment without disrupting vehicle operations. What solution should they use?",
    "back": "Use Vertex AI Model Registry for version control and model management. Deploy models using Fleet Management tools (Anthos or GKE Connect) for centralized management across distributed devices. Use Cloud Storage for content synchronization (distributing large binary assets like models and map data over intermittent connections). Cloud Storage is globally accessible and cost-effective for distributing large files to the vehicle fleet.",
    "tags": ["case-study", "knightmotives", "edge-computing", "vertex-ai", "model-registry", "fleet-management", "cloud-storage", "edge-ml"]
  },
  {
    "id": "gcp-case-study-040",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They plan to monetize vehicle data by creating a partner ecosystem, allowing third-party developers to access vehicle telemetry and predictive maintenance models via APIs. This requires throttling, key management, monetization, and integration with underlying GKE microservices. What solution should they use?",
    "back": "Use Apigee as a unified API gateway for the ecosystem. Apigee provides advanced traffic management (throttling, spike arrest), developer portal (sign-up, documentation, key management), monetization features (billing, usage tracking), and seamless integration with backend GKE microservices. Apigee is Google Cloud's enterprise-grade API management platform designed specifically for partner ecosystems and API monetization.",
    "tags": ["case-study", "knightmotives", "api-management", "apigee", "monetization", "partner-ecosystem", "developer-portal"]
  },
  {
    "id": "gcp-case-study-041",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: They need to provide external partners with private, high-throughput access to their data analysis services hosted in a Shared VPC. Partners must not traverse the public internet, and KnightMotives must not expose their internal network to partners' VPCs. What solution should they use?",
    "back": "Use Private Service Connect (PSC) to expose the KnightMotives service as a private endpoint to partners' separate VPCs. PSC keeps traffic on Google's private backbone (no public internet), is service-oriented (partners only see the exposed service endpoint, not the entire VPC), and doesn't require IP address coordination. This is the standard Google Cloud solution for secure partner access without exposing internal networks.",
    "tags": ["case-study", "knightmotives", "networking", "private-service-connect", "partner-access", "security", "vpc"]
  },
  {
    "id": "gcp-case-study-042",
    "category": "GCP",
    "subcategory": "Case Studies",
    "front": "KnightMotives: For the majority of sensor data (400 TB to 1 PB per day) that is compressed and uploaded daily via batch jobs, they need a system that efficiently stores massive structured data, runs complex SQL queries for historical analysis, and scales processing capacity based on daily ingestion load. What solution should they use?",
    "back": "Store data in a single, partitioned table in BigQuery. Configure BigQuery Autoscaling (Slots) to manage variable daily loads. BigQuery is the serverless data warehouse for petabyte-scale structured data analysis using SQL. Partitioning optimizes query costs and performance on massive datasets. Automatic slot management handles variable daily loads (400 TB to 1 PB) without manual cluster management, providing the simplest and most scalable solution.",
    "tags": ["case-study", "knightmotives", "bigquery", "data-warehouse", "batch-processing", "autoscaling", "petabyte-scale"]
  }
]
